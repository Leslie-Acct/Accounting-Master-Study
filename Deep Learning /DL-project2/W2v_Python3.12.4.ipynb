{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c239dd3b",
      "metadata": {},
      "source": [
        "**Python 3.12.4**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bcad8d1",
      "metadata": {},
      "source": [
        "\n",
        "Multiple sentiment classification approaches:\n",
        "\tâ€¢\tDictionary-based sentiment scoring\n",
        "\tâ€¢\tTF-IDF + Logistic Regression\n",
        "\tâ€¢\tRNN with randomly initialized embeddings\n",
        "\tâ€¢\tRNN with pre-trained W2v embeddings\n",
        "\tâ€¢\tDistilBERT (Huggingface pipeline)\n",
        "\tâ€¢\tFine-tuned DistilBERT using Trainer API"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d207339",
      "metadata": {},
      "source": [
        "The dataset files (btc_tweets_train.parquet.gzip, btc_tweets_test.parquet.gzip) and the pretrained Word2Vec model w2v_imdb_full_d100_e500.model are assumed to be located in the same directory as this notebook.**Download link:**\n",
        "https://drive.google.com/file/d/1J1edk-jyPsO0PCiY-NEFiRxo-PIYART3/view?usp=drive_link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ki0IPN9K_A5T",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ki0IPN9K_A5T",
        "outputId": "20764f96-a29c-4b42-edd4-caacd18b964e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
            "Requirement already satisfied: pyarrow in /opt/anaconda3/lib/python3.12/site-packages (21.0.0)\n",
            "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (3.8.4)\n",
            "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.12/site-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.4.2)\n",
            "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (3.8.1)\n",
            "Requirement already satisfied: emoji in /opt/anaconda3/lib/python3.12/site-packages (2.14.1)\n",
            "Requirement already satisfied: unidecode in /opt/anaconda3/lib/python3.12/site-packages (1.2.0)\n",
            "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.54.1)\n",
            "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.7.1)\n",
            "Requirement already satisfied: tf-keras in /opt/anaconda3/lib/python3.12/site-packages (2.19.0)\n",
            "Requirement already satisfied: tensorflow in /opt/anaconda3/lib/python3.12/site-packages (2.19.0)\n",
            "Requirement already satisfied: xgboost in /opt/anaconda3/lib/python3.12/site-packages (3.0.2)\n",
            "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.12/site-packages (4.3.3)\n",
            "Requirement already satisfied: pysentiment2 in /opt/anaconda3/lib/python3.12/site-packages (0.1.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (4.12.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (10.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (2.2.0)\n",
            "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.2.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.9.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.3.5)\n",
            "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /opt/anaconda3/lib/python3.12/site-packages (1.9.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (23.2)\n",
            "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (5.9.0)\n",
            "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.0.1)\n",
            "Requirement already satisfied: torch>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (2.7.1)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.34.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2024.3.1)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.32.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (1.1.5)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (69.5.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/wanghaiyang/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/wanghaiyang/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/wanghaiyang/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /Users/wanghaiyang/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/wanghaiyang/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyarrow==12.0.1\n",
            "  Using cached pyarrow-12.0.1.tar.gz (1.0 MB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h\u001b[33m  WARNING: Requested pyarrow==12.0.1 from https://files.pythonhosted.org/packages/c5/68/d3410e975bebbf5be00c1238d0418345d8ec5d88b7a6c102211a1c967edd/pyarrow-12.0.1.tar.gz, but installing version 12.0.0\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/c5/68/d3410e975bebbf5be00c1238d0418345d8ec5d88b7a6c102211a1c967edd/pyarrow-12.0.1.tar.gz (from https://pypi.org/simple/pyarrow/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested pyarrow==12.0.1 from https://files.pythonhosted.org/packages/c5/68/d3410e975bebbf5be00c1238d0418345d8ec5d88b7a6c102211a1c967edd/pyarrow-12.0.1.tar.gz has inconsistent version: expected '12.0.1', but metadata has '12.0.0'\u001b[0m\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pyarrow==12.0.1 (from versions: 0.9.0, 0.10.0, 0.11.0, 0.11.1, 0.12.0, 0.12.1, 0.13.0, 0.14.0, 0.15.1, 0.16.0, 0.17.0, 0.17.1, 1.0.0, 1.0.1, 2.0.0, 3.0.0, 4.0.0, 4.0.1, 5.0.0, 6.0.0, 6.0.1, 7.0.0, 8.0.0, 9.0.0, 10.0.0, 10.0.1, 11.0.0, 12.0.0, 12.0.1, 13.0.0, 14.0.0, 14.0.1, 14.0.2, 15.0.0, 15.0.1, 15.0.2, 16.0.0, 16.1.0, 17.0.0, 18.0.0, 18.1.0, 19.0.0, 19.0.1, 20.0.0, 21.0.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pyarrow==12.0.1\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.32.2)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.3.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.34.3)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.9.3)\n",
            "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# === 1. Package Installation ===\n",
        "!pip install pandas numpy pyarrow matplotlib seaborn scikit-learn nltk emoji unidecode transformers torch tf-keras tensorflow xgboost gensim pysentiment2 beautifulsoup4\n",
        "!pip install 'accelerate>=0.26.0'\n",
        "\n",
        "# === 2. NLTK Setup ===\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# === 3. Standard Libraries ===\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import html\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# === 4. Text Preprocessing ===\n",
        "from bs4 import BeautifulSoup\n",
        "import emoji\n",
        "from unidecode import unidecode\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# === 5. Dictionary-Based Sentiment Analysis ===\n",
        "import pysentiment2 as ps\n",
        "dc = ps.HIV4()\n",
        "\n",
        "# === 6. Classical Machine Learning ===\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "# === 7. Keras / TensorFlow Deep Learning Models ===\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, GRU, LSTM, Dropout, Flatten, Bidirectional\n",
        "from keras.initializers import Constant\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential as TFSequential\n",
        "from tensorflow.keras.layers import Embedding as TFEmbedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Bidirectional, GRU, LSTM, Dense, Dropout\n",
        "\n",
        "# === 8. Word Embeddings (Pre-trained) ===\n",
        "from gensim.models import KeyedVectors\n",
        "w2v_kv = KeyedVectors.load_word2vec_format(\"w2v_imdb_full_d100_e500.model\", binary=False)\n",
        "\n",
        "\n",
        "# === 9. Huggingface Transformers Setup ===\n",
        "from transformers import (\n",
        "    DistilBertTokenizerFast,\n",
        "    DistilBertForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    pipeline\n",
        ")\n",
        "from transformers import pipeline\n",
        "\n",
        "## fine-tune for the Huggingface pipeline\n",
        "!pip install pyarrow==12.0.1 --force-reinstall\n",
        "!pip install datasets\n",
        "from datasets import Dataset\n",
        "from datasets import ClassLabel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41b1fde5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "41b1fde5",
        "outputId": "5f19cdd8-0ddc-4ebe-f1a5-8cb23f556398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bound method NDFrame.head of                                                               hashtags  \\\n",
            "tweet ID                                                                 \n",
            "1641861708246552576                                      [crypto, btc]   \n",
            "1641861783898972167                                 [Bitcoin, Bitcoin]   \n",
            "1641862152532418562  [Giveaway, BTC, SolanaGiveaways, Giveaway, Air...   \n",
            "1641862338369183753  [EOS, USDT, BTC, crypto, Bitcoin, etherium, Bi...   \n",
            "1641862430434131968                        [BTC, ETH, BSC, GroveToken]   \n",
            "...                                                                ...   \n",
            "1641952735653421056                                          [Bitcoin]   \n",
            "1641952895112290305                                          [Bitcoin]   \n",
            "1641952949763973122                       [STAMPS, XCP, ORDINALS, BTC]   \n",
            "1641953124981284864  [Bitcoin, Decentralization, blockchain, ChatGP...   \n",
            "1641953216999968769      [Web3, BTC, blockchain, seer, cryptocurrency]   \n",
            "\n",
            "                                                               content  \\\n",
            "tweet ID                                                                 \n",
            "1641861708246552576  #crypto $crypto #btc \\nI am Chinese crypto alp...   \n",
            "1641861783898972167  #Bitcoin would have to fall another 80% to rea...   \n",
            "1641862152532418562  #Giveaway $1000 Matic in 3Days\\n\\nðŸ†To win\\n1ï¸âƒ£...   \n",
            "1641862338369183753  Up or Down?\\n\\n!!! $EOS #EOS !!!\\n\\nVS\\n\\n$USD...   \n",
            "1641862430434131968  Mid Day Mix-up is LIVE! Never know who might s...   \n",
            "...                                                                ...   \n",
            "1641952735653421056  The US government puts a guy in jail for 2 lif...   \n",
            "1641952895112290305  #Bitcoin Embrace it âš¡ï¸ â‚¿ â‚¿ â‚¿âš¡ï¸ https://t.co/lq...   \n",
            "1641952949763973122  â˜†â˜†â˜†ONWARD UPWARD BABE\\ni love stamps i love or...   \n",
            "1641953124981284864  Does privacy matters to you? \\n\\nIf so, should...   \n",
            "1641953216999968769  Good morning guys!ðŸŒžðŸŒž\\n\\nWe are very excited to...   \n",
            "\n",
            "                           username            user_displayname  sentiment  \n",
            "tweet ID                                                                    \n",
            "1641861708246552576       huahuayjy      èŠ±èŠ±ç ”ç©¶é™¢ | Crypto AlphaðŸ‡¨ðŸ‡³       True  \n",
            "1641861783898972167    luke_broyles                Luke Broyles      False  \n",
            "1641862152532418562    cryptomarsdo                 Crypto Mars       True  \n",
            "1641862338369183753    andreyukrnet              Andrey Ukraine       True  \n",
            "1641862430434131968      JustAman04             Justin Anderson       True  \n",
            "...                             ...                         ...        ...  \n",
            "1641952735653421056    IncomeSharks                IncomeSharks      False  \n",
            "1641952895112290305    sailortrades                sailortrades       True  \n",
            "1641952949763973122        findbleh                      ðŸ¤ bleh       True  \n",
            "1641953124981284864  SigmaAlphaGod6  â‚¿Lâ“§C STAR 6. (ALPHANOMICS)       True  \n",
            "1641953216999968769  SeerFoundation                   SEER.web3       True  \n",
            "\n",
            "[500 rows x 5 columns]>\n",
            "<bound method NDFrame.head of                                                               hashtags  \\\n",
            "tweet ID                                                                 \n",
            "1641579121972236290  [Bitcoin, Bitcoin, BTC, Bitcoin, BTC, SHIB, HO...   \n",
            "1641579176171016194                 [Bitcoin, bitcoinordinals, crypto]   \n",
            "1641579486071390208  [BTC, SHIB, HOGE, SAITAMA, BNB, DOGE, ETH, Bab...   \n",
            "1641579537103302656                                              [BTC]   \n",
            "1641579588399804418                                          [Bitcoin]   \n",
            "...                                                                ...   \n",
            "1641860739596967946                                          [bitcoin]   \n",
            "1641861088483368964                                     [BITCOIN, LFG]   \n",
            "1641861498451402753  [MASK, maskusdt, maskbusd, maskbtc, sxp, bitco...   \n",
            "1641861541774626816                      [Bitcoin, Ethereum, BTC, ETH]   \n",
            "1641861677149822976                                          [Bitcoin]   \n",
            "\n",
            "                                                               content  \\\n",
            "tweet ID                                                                 \n",
            "1641579121972236290  $Bitcoin TO $100,000 SOONER THAN YOU THINKâ€¼ï¸ðŸ’¯ðŸ™...   \n",
            "1641579176171016194  Alright I have my rares. Who else is grabbing ...   \n",
            "1641579486071390208  Bitcoin (BTC) Targets Over $100,000 as This Im...   \n",
            "1641579537103302656  ðŸ“¢ Xverse Web-based pool is live:\\n\\nâ€¢Update @x...   \n",
            "1641579588399804418  Yesterday, a Bitcoin projection was displayed ...   \n",
            "...                                                                ...   \n",
            "1641860739596967946  Great listen!  @CharlieShrem is a $btc legend ...   \n",
            "1641861088483368964  Last chance below 30k #BITCOINâ€¯â€¯â€¯ #LFG \\nSet a...   \n",
            "1641861498451402753  My roadmap for #MASK continues in the same way...   \n",
            "1641861541774626816  Just a few years ago, #Bitcoin  was the talk o...   \n",
            "1641861677149822976  If the government are allowed to sell all the ...   \n",
            "\n",
            "                            username     user_displayname  sentiment  \n",
            "tweet ID                                                              \n",
            "1641579121972236290      BezosCrypto           SHIB Bezos       True  \n",
            "1641579176171016194      spartantc81            SpartanTC       True  \n",
            "1641579486071390208      BezosCrypto           SHIB Bezos       True  \n",
            "1641579537103302656     godfred_xcuz        Algorithm.btc       True  \n",
            "1641579588399804418      goddess81oo         she is lucky       True  \n",
            "...                              ...                  ...        ...  \n",
            "1641860739596967946      kylelangham               Kyle âˆž       True  \n",
            "1641861088483368964      cryptoal3rt         Crypto Alert       True  \n",
            "1641861498451402753  CryptoBitcoinCT    Crypto Bitcoin CT       True  \n",
            "1641861541774626816       CryptoEntj         ðŸ‘‰Crypto ENTJ       True  \n",
            "1641861677149822976     TheShibLord1  ðŸ’¸TheLordOfTheShillðŸ’¸      False  \n",
            "\n",
            "[1500 rows x 5 columns]>\n",
            "Index(['hashtags', 'content', 'username', 'user_displayname', 'sentiment'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Load Data\n",
        "\n",
        "df_test= pd.read_parquet('btc_tweets_test.parquet.gzip')\n",
        "print(df_test.head)\n",
        "df_train = pd.read_parquet('btc_tweets_train.parquet.gzip')\n",
        "print(df_train.head)\n",
        "print(df_test.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7lzKGdSCdUDV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lzKGdSCdUDV",
        "outputId": "8703d47f-057d-4265-87e8-0fec3a00531b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentiment\n",
            "True     1220\n",
            "False     280\n",
            "Name: count, dtype: int64\n",
            "sentiment\n",
            "True     404\n",
            "False     96\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df_train['sentiment'].value_counts())\n",
        "print(df_test['sentiment'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ayccvsl0gr8T",
      "metadata": {
        "id": "ayccvsl0gr8T"
      },
      "outputs": [],
      "source": [
        "# Utility Functions\n",
        "##  Text Preprocessing Utility\n",
        "\n",
        "\n",
        "class preprocess_text:\n",
        "    \"\"\"\n",
        "    A clean, modular text preprocessor for web-scraped or informal social media text.\n",
        "    Includes HTML, emoji, URL removal, lemmatization, and optional stopword filtering.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, remove_stopwords=True, lemmatize=True):\n",
        "        self.remove_stopwords = remove_stopwords\n",
        "        self.lemmatize = lemmatize\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def clean(self, text: str) -> str:\n",
        "        # 1. Lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # 2. Remove HTML tags\n",
        "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "        # 3. Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "\n",
        "        # 4. Remove emojis\n",
        "        text = emoji.replace_emoji(text, replace='')\n",
        "\n",
        "        # 5. Remove punctuation\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "        # 6. Remove accents (e.g., Ã© â†’ e)\n",
        "        text = unidecode(text)\n",
        "\n",
        "        # 7. Remove hashtags and mentions\n",
        "        text = re.sub(r'[@#]\\w+', '', text)\n",
        "\n",
        "        # 8. Remove extra whitespaces\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # 9. Tokenize\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "\n",
        "        # 10. Stopword removal\n",
        "        if self.remove_stopwords:\n",
        "            tokens = [word for word in tokens if word not in self.stop_words]\n",
        "\n",
        "        # 11. Lemmatization\n",
        "        if self.lemmatize:\n",
        "            tokens = [self.lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "\n",
        "#  Evaluation Functions (Shared)\n",
        "\n",
        "def assess_binary_classifier(ytest, yhat, cut_off=0.5, plot_roc=True):\n",
        "\n",
        "    # Calculate discrete class predictions\n",
        "\n",
        "    yhat_c = np.where(yhat>cut_off, 1, 0)\n",
        "\n",
        "    # Calculate classification accuracy and AUC\n",
        "    acc = accuracy_score(ytest, yhat_c)\n",
        "    auc = roc_auc_score(ytest, yhat)\n",
        "    rec = recall_score (ytest, yhat_c)\n",
        "    f1 = f1_score (ytest, yhat_c)\n",
        "    prec = precision_score(ytest, yhat_c)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cmat = confusion_matrix(ytest, yhat_c)\n",
        "\n",
        "    # ROC analysis\n",
        "    if plot_roc==True:\n",
        "        fpr, tpr, _ = roc_curve(ytest, yhat)\n",
        "        plt.plot(fpr,tpr, label=\"AUC={:.4}\".format(auc));\n",
        "        plt.plot([0, 1], [0, 1], \"r--\")\n",
        "        plt.ylabel('True positive rate')\n",
        "        plt.xlabel('False positive rate')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.show();\n",
        "\n",
        "    return(auc, acc, cmat, prec, rec, f1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Dictionary & TF-IDF Baseline\n",
        "\n",
        "def get_sentiment_score(text):\n",
        "    score = round(dc.get_score(dc.tokenize(text))['Polarity'], 2)\n",
        "    return score\n",
        "\n",
        "\n",
        "\n",
        "def get_tfidf_vectorizer():\n",
        "    def dummy_fun(doc):\n",
        "        return doc\n",
        "    return TfidfVectorizer(\n",
        "        analyzer='word',\n",
        "        tokenizer=dummy_fun,\n",
        "        preprocessor=dummy_fun,\n",
        "        token_pattern=None)\n",
        "\n",
        "\n",
        "\n",
        "# RNN Model Builder\n",
        "def build_bidirectional_rnn_classifier(\n",
        "    vocab_size,\n",
        "    embedding_dim,\n",
        "    input_length,\n",
        "    rnn_type='gru',\n",
        "    hidden_units=16,\n",
        "    learning_rate=0.001,\n",
        "    embedding_weights=None,\n",
        "    trainable_embedding=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Build a Bidirectional GRU or LSTM sentiment classifier.\n",
        "\n",
        "    Parameters:\n",
        "    - vocab_size: int, size of vocabulary (NUM_WORDS)\n",
        "    - embedding_dim: int, size of word embedding vectors\n",
        "    - input_length: int, length of input sequences after padding\n",
        "    - rnn_type: str, 'gru' or 'lstm'\n",
        "    - hidden_units: int, number of hidden units in RNN layer\n",
        "    - learning_rate: float, learning rate for optimizer\n",
        "\n",
        "    Returns:\n",
        "    - Compiled Keras model\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    # Embedding\n",
        "    if embedding_weights is not None:\n",
        "        model.add(Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embedding_dim,\n",
        "            input_length=input_length,\n",
        "            embeddings_initializer=Constant(embedding_weights),\n",
        "            trainable=trainable_embedding\n",
        "        ))\n",
        "    else:\n",
        "        model.add(Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embedding_dim,\n",
        "            input_length=input_length\n",
        "        ))\n",
        "\n",
        "\n",
        "    if rnn_type.lower() == 'gru':\n",
        "        model.add(Bidirectional(GRU(hidden_units, activation='relu')))\n",
        "    elif rnn_type.lower() == 'lstm':\n",
        "        model.add(Bidirectional(LSTM(hidden_units, activation='relu')))\n",
        "    else:\n",
        "        raise ValueError(\"rnn_type must be either 'gru' or 'lstm'\")\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# RNN Evaluation Function\n",
        "\n",
        "def diag_nn(model, story, x_ts, y_ts, plot_roc=True, plot_loss=True):\n",
        "\n",
        "\n",
        "    # Calculate test set predictions\n",
        "    yhat = model.predict(x_ts).flatten()\n",
        "\n",
        "    # Calling our helper for classifier evaluation\n",
        "    auc, acc ,cmat ,prec, rec, f1 = assess_binary_classifier(y_ts, yhat, cut_off=0.5, plot_roc=plot_roc)\n",
        "    print(\"NN test set performance:\\tAUC={:.4f}\\tAccuracy={:.4f}\".format(auc, acc))\n",
        "    print('Confusion matrix:')\n",
        "    print(cmat)\n",
        "\n",
        "    if plot_loss:\n",
        "        plt.plot(story.history['loss'])\n",
        "        plt.plot(story.history['val_loss'])\n",
        "        plt.title('loss evolution')\n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train', 'validation'], loc='upper left')\n",
        "        plt.show()\n",
        "\n",
        "    return (auc, acc ,cmat, prec, rec, f1)\n",
        "\n",
        "###Embedding Matrix Builder\n",
        "\n",
        "def get_embedding_matrix(tokenizer, pretrain, vocab_size, verbose=0):\n",
        "\n",
        "\n",
        "    dim = 0\n",
        "\n",
        "    if isinstance(pretrain, KeyedVectors) or isinstance(pretrain, Word2VecKeyedVectors):\n",
        "        dim = pretrain.vector_size\n",
        "    elif isinstance(pretrain, dict):\n",
        "        dim = next(iter(pretrain.values())).shape[0]  # get embedding of an arbitrary word\n",
        "    else:\n",
        "        raise Exception('{} is not supported'.format(type(pretrain)))\n",
        "\n",
        "\n",
        "    # Initialize embedding matrix\n",
        "    emb_mat = np.zeros((vocab_size, dim))\n",
        "\n",
        "\n",
        "    oov_words = []\n",
        "\n",
        "    v = len(tokenizer.word_index)\n",
        "    start = time.time()\n",
        "    print('Start embedding process for {} words.'.format(v), flush=True)\n",
        "\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        # try-catch together with a zero-initilaized embedding matrix achieves our rough fix for oov words\n",
        "        try:\n",
        "            emb_mat[i] = pretrain[word]\n",
        "        except:\n",
        "            oov_words.append(word)\n",
        "        # Some output that the method is still alive\n",
        "        if i % 5000 == 0 and verbose>0:\n",
        "            print('{}/{} words in {} sec'.format(i, v, (time.time()-start)), flush=True)\n",
        "\n",
        "\n",
        "    print('Created embedding matrix of shape {} in {} min '.format(emb_mat.shape, (time.time()-start)/60))\n",
        "\n",
        "    print('Encountered {} out-of-vocabulary words.'.format(len(oov_words)))\n",
        "    return (emb_mat, oov_words)\n",
        "\n",
        "\n",
        "\n",
        "# Huggingface Inference Pipeline\n",
        "\n",
        "\n",
        "\n",
        "def pipeline_classify(data):\n",
        "\n",
        "    labels = []\n",
        "    scores = []\n",
        "    for text in data:\n",
        "        result = classifier(text, truncation=True)\n",
        "        label = result[0]['label']\n",
        "        score = result[0]['score']\n",
        "\n",
        "        positive_prob = score if label.upper() == 'POSITIVE' else 1 - score\n",
        "        labels.append(label)\n",
        "        scores.append(positive_prob)\n",
        "    return labels, scores\n",
        "\n",
        "\n",
        "### Fine-tune DistilBERT\n",
        "\n",
        "\n",
        "from transformers import Trainer, TrainingArguments, DistilBertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def fine_tune_distilbert(\n",
        "    train_dataset,\n",
        "    test_dataset,\n",
        "    model_name=\"distilbert-base-uncased\",\n",
        "    num_labels=2,\n",
        "    seed=42,\n",
        "    num_train_epochs=1,\n",
        "    train_batch_size=16,\n",
        "    eval_batch_size=32,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.0,\n",
        "    output_dir=\"./results\",\n",
        "    logging_dir=\"./logs\",\n",
        "    return_trainer=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Fine-tune a pre-trained DistilBERT model on binary sentiment classification task.\n",
        "\n",
        "    Parameters:\n",
        "    - train_dataset, test_dataset: tokenized Huggingface Dataset\n",
        "    - model_name: huggingface model name\n",
        "    - num_labels: number of output classes (2 for binary)\n",
        "    - seed: random seed\n",
        "    - num_train_epochs: training epochs\n",
        "    - train_batch_size: training batch size\n",
        "    - eval_batch_size: evaluation batch size\n",
        "    - learning_rate: Adam learning rate\n",
        "    - weight_decay: optional weight decay\n",
        "    - output_dir: folder to save output (optional)\n",
        "    - logging_dir: log folder (optional)\n",
        "    - return_trainer: if True, return trainer + model; else only model\n",
        "\n",
        "    Returns:\n",
        "    - model (and optionally trainer)\n",
        "    \"\"\"\n",
        "\n",
        "    # Disable wandb logs\n",
        "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "    # Load model\n",
        "    model = DistilBertForSequenceClassification.from_pretrained(\n",
        "        model_name, num_labels=num_labels\n",
        "    )\n",
        "\n",
        "    # Define metrics\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        preds = np.argmax(logits, axis=1)\n",
        "        return {\n",
        "            \"accuracy\": accuracy_score(labels, preds),\n",
        "            \"precision\": precision_score(labels, preds),\n",
        "            \"recall\": recall_score(labels, preds),\n",
        "            \"f1\": f1_score(labels, preds)\n",
        "        }\n",
        "\n",
        "\n",
        "    # Define training arguments (updated)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        logging_dir=logging_dir,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        per_device_train_batch_size=train_batch_size,\n",
        "        per_device_eval_batch_size=eval_batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=weight_decay,\n",
        "        \n",
        "        logging_steps=10,\n",
        "        seed=seed,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "    # Build trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    return (trainer, model) if return_trainer else model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "KtxwRIm2hYPL",
      "metadata": {
        "id": "KtxwRIm2hYPL"
      },
      "outputs": [],
      "source": [
        "#  data frame for the whole assessment\n",
        "df_scores = pd.DataFrame(index=['Accuracy', 'Precision', 'Recall', 'F1', 'AUC'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7ffWNwkof5s-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ffWNwkof5s-",
        "outputId": "a2435825-2742-4bcf-c42e-512e95cd0f45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 2.6785714285714284, 1: 0.6147540983606558}\n"
          ]
        }
      ],
      "source": [
        "#imbalance data\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# compute class weight\n",
        "y_train = df_train['sentiment'] # Moved this line here\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "# convert to dictionary\n",
        "class_weights = {0: class_weights[0], 1: class_weights[1]}\n",
        "print(class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "zo5wpVTAnhCK",
      "metadata": {
        "id": "zo5wpVTAnhCK"
      },
      "outputs": [],
      "source": [
        "df_train['sentiment'] = df_train['sentiment'].astype(int)\n",
        "df_test['sentiment'] = df_test['sentiment'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8e109f35",
      "metadata": {
        "id": "8e109f35"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/7b/m_fkmvpn11j4t6w93zx70w680000gn/T/ipykernel_12208/3741683227.py:22: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
            "/var/folders/7b/m_fkmvpn11j4t6w93zx70w680000gn/T/ipykernel_12208/3741683227.py:22: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
          ]
        }
      ],
      "source": [
        "# preprocess the testing and training set\n",
        "cleaner = preprocess_text()\n",
        "\n",
        "df_train['clean_content'] = df_train['content'].apply(lambda x: cleaner.clean(x))\n",
        "df_test['clean_content'] = df_test['content'].apply(lambda x: cleaner.clean(x))\n",
        "\n",
        "\n",
        "#df_train['clean_content'] = df_train['content'].apply(preprocess_text)\n",
        "#df_test['clean_content'] = df_test['content'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c7eef2e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c7eef2e7",
        "outputId": "1fb98042-ba42-4beb-f884-dc14d44c7fab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== Sample 1 ====\n",
            "Original:  #crypto $crypto #btc \n",
            "I am Chinese crypto alpha, although my English is not good, but I will try to send some English tweets!  Let me see if there are messages of support from foreign fansï¼\n",
            "Cleaned :  crypto crypto btc chinese crypto alpha although english good try send english tweet let see message support foreign fan !\n",
            "==== Sample 2 ====\n",
            "Original:  #Bitcoin would have to fall another 80% to reach the lows of 3 years ago.\n",
            "\n",
            "Rates have already skyrocketed, most crypto platforms (including FTX) died, and sentiment is dead as most think itâ€™s over.\n",
            "\n",
            "Even if #Bitcoin falls 50% to $14k new lows (which I doubt), this is remarkable.\n",
            "Cleaned :  bitcoin would fall another 80 reach low 3 year ago rate already skyrocketed crypto platform including ftx died sentiment dead think 's even bitcoin fall 50 14k new low doubt remarkable\n",
            "==== Sample 3 ====\n",
            "Original:  #Giveaway $1000 Matic in 3Days\n",
            "\n",
            "ðŸ†To win\n",
            "1ï¸âƒ£ Follow  @matic\n",
            "2ï¸âƒ£ Like &amp; RT\n",
            "3ï¸âƒ£ Tag 3 Friends \n",
            "\n",
            "#BTCâ€¯â€¯#SolanaGiveaways #Giveaway #Airdrop #Matic #Ripple #GiveawayAlert https://t.co/ysklxXKPbM\n",
            "Cleaned :  giveaway 1000 matic 3days win follow matic like rt tag 3 friend btc solanagiveaways giveaway airdrop matic ripple giveawayalert\n",
            "==== Sample 4 ====\n",
            "Original:  Up or Down?\n",
            "\n",
            "!!! $EOS #EOS !!!\n",
            "\n",
            "VS\n",
            "\n",
            "$USDT #USDT \n",
            "\n",
            "AND\n",
            "\n",
            "$BTC #BTC \n",
            "\n",
            "#crypto #Bitcoin #etherium #Binance https://t.co/k8BaRGJcWa\n",
            "Cleaned :  eos eos v usdt usdt btc btc crypto bitcoin etherium binance\n",
            "==== Sample 5 ====\n",
            "Original:  Mid Day Mix-up is LIVE! Never know who might stop by! #BTC #ETH #BSC #GroveToken\n",
            "\n",
            "https://t.co/zV22py2sxp\n",
            "Cleaned :  mid day mixup live never know might stop btc eth bsc grovetoken\n",
            "==== Sample 6 ====\n",
            "Original:  #BitcoinÂ Bear Market already ended! ðŸš¨\n",
            "\n",
            "@rovercrc @AurelienOhayon \n",
            "\n",
            "#Bitcoin Bull Market Officially Begins â€¼ï¸\n",
            "\n",
            "WAIT FOR #ETH #SHIB #AltCoins TO FOLLOW , \n",
            "PATIENCE IS THE KEY\n",
            "\n",
            "$SNAP $HOOD $CORZ $BKKT $AMC $NIO \n",
            "\n",
            "#BTCÂ #SHIB \n",
            "#HOGE #SAITAMA #BNBÂ Â #DOGE #ETH #BabyFloki #AltCoinSeason https://t.co/NFmaen8pjW\n",
            "Cleaned :  bitcoin bear market already ended rovercrc aurelienohayon bitcoin bull market officially begin wait eth shib altcoins follow patience key snap hood corz bkkt amc nio btc shib hoge saitama bnb doge eth babyfloki altcoinseason\n",
            "==== Sample 7 ====\n",
            "Original:  US government has announced its intention to sell an additional 41,500 #BTC this year. To minimize the potential impact on the market, I would like to offer to purchase these BTC at a 10% discount for an OTC deal.\n",
            "Cleaned :  u government announced intention sell additional 41500 btc year minimize potential impact market would like offer purchase btc 10 discount otc deal\n",
            "==== Sample 8 ====\n",
            "Original:  Spend your #Bitcoin and replace 2x\n",
            "Cleaned :  spend bitcoin replace 2x\n",
            "==== Sample 9 ====\n",
            "Original:  #Hex launched &amp; gained 11,900%\n",
            "(Nomics .com)\n",
            "\n",
            "#Pulsechain 2 do- 3X of $HEX \n",
            "\n",
            "Why?\n",
            "\n",
            "#PLS -Already has 40k user \n",
            "75-85k new Buyer\n",
            "Over 60 projects ready\n",
            "\n",
            "Almost 80% supply be locked\n",
            "By #OG #Staking #Validator \n",
            "\n",
            "Retweet â¤ï¸ $PLS ppl\n",
            "\n",
            "#BTC #BNB #HEX #PLS #PLSX #ETH #Crypto #XRP #USDT https://t.co/ZG3OlPOyUy\n",
            "Cleaned :  hex launched gained 11900 nomics com pulsechain 2 3x hex pls already 40k user 7585k new buyer 60 project ready almost 80 supply locked og staking validator retweet pls ppl btc bnb hex pls plsx eth crypto xrp usdt\n",
            "==== Sample 10 ====\n",
            "Original:  #Bitcoin : Your Shield Against Inflation and Economic Volatility https://t.co/gDlEFkfqpV\n",
            "Cleaned :  bitcoin shield inflation economic volatility\n",
            "==== Sample 11 ====\n",
            "Original:  Another day, another emission reductionðŸ”¥\n",
            "\n",
            "$ONYX emissions have been reduced to 0.4 per secondðŸ’Ž\n",
            "\n",
            "No better time than the present to get $ONYX to ern #RealYield in #USDT , #Bitcoin , &amp; $ARB âœ¨\n",
            "\n",
            "#DeFi #Arbitrum #Crypto\n",
            "Cleaned :  another day another emission reduction onyx emission reduced 04 per second better time present get onyx ern realyield usdt bitcoin arb defi arbitrum crypto\n",
            "==== Sample 12 ====\n",
            "Original:  Watched &amp; enjoyed the video again @BossCatRC ðŸ”¥\n",
            "Go watch it to explore this project. \n",
            "1 announcement was they are creating there own raffle system right now. Where you can use your staked bosspoints. \n",
            "\n",
            "#btc #Bitcoin\n",
            "#OrdinalsNFTs #Staking #coldwallet \n",
            "\n",
            "https://t.co/SDKLtiap4F\n",
            "Cleaned :  watched enjoyed video bosscatrc go watch explore project 1 announcement creating raffle system right use staked bosspoints btc bitcoin ordinalsnfts staking coldwallet\n",
            "==== Sample 13 ====\n",
            "Original:  Break out of the Matrix with #Bitcoin https://t.co/rNl8V42LXr\n",
            "Cleaned :  break matrix bitcoin\n",
            "==== Sample 14 ====\n",
            "Original:  ðŸ•°ï¸ Patience is key when it comes to #Bitcoin adoption. \n",
            "\n",
            "Itâ€™s unfair to believe that #Bitcoin adoption can happen immediately. Look how long it took for the #euro to be adopted in Europe; it was forced on people. â€˜Bitcoin is a much better storyâ€™ @paoloardoino #FinancialFreedom.â³ https://t.co/ug0W59MMCg\n",
            "Cleaned :  patience key come bitcoin adoption 's unfair believe bitcoin adoption happen immediately look long took euro adopted europe forced people 'bitcoin much better story ' paoloardoino financialfreedom\n",
            "==== Sample 15 ====\n",
            "Original:  Alpha for the weekend:\n",
            "\n",
            "Early #btc STAMPS, first priority will be given to XCPinata Collectors &amp; PunchPuff holders\n",
            "\n",
            "Thank you for collectingðŸ™\n",
            "\n",
            "Allowlist in discord channelðŸ‘‡\n",
            "ðŸ”— https://t.co/bX2tRBPevM\n",
            "\n",
            "PunchPuff MintðŸ‘‡\n",
            "ðŸ”— https://t.co/DVhfQihD7r\n",
            "\n",
            "#STAMPS #bitcoin #bitcoinstamps\n",
            "Cleaned :  alpha weekend early btc stamp first priority given xcpinata collector punchpuff holder thank collecting allowlist discord channel punchpuff mint stamp bitcoin bitcoinstamps\n",
            "==== Sample 16 ====\n",
            "Original:  #Bitcoin / $BTC\n",
            "\n",
            "Resume the pump https://t.co/nfd10rwmzd\n",
            "Cleaned :  bitcoin btc resume pump\n",
            "==== Sample 17 ====\n",
            "Original:  Be ready for next hidden #gem #rwa coin next 12 to 16 hours will make great profit \n",
            "#btc move 2x 3x in year we move xxx in days https://t.co/Gf01pxVoTi\n",
            "Cleaned :  ready next hidden gem rwa coin next 12 16 hour make great profit btc move 2x 3x year move xxx day\n",
            "==== Sample 18 ====\n",
            "Original:  ðŸ—“ï¸2023 will be our year! ðŸ’¹\n",
            "\n",
            "I believe we'll see a massive #BTC pump and much more #altcoins with 100x pumps! ðŸš€\n",
            "Cleaned :  2023 year believe well see massive btc pump much altcoins 100x pump\n",
            "==== Sample 19 ====\n",
            "Original:  JUST IN: Tron CEO Justin Sun, who is also under investigation for fraud by SEC, says he wants to purchase $110,000,000+ of #Bitcoin from US government; who is planning to sell.\n",
            "Cleaned :  tron ceo justin sun also investigation fraud sec say want purchase 110000000 bitcoin u government planning sell\n",
            "==== Sample 20 ====\n",
            "Original:  Solid first principles of $XEN \n",
            "1ï¸âƒ£ Decentralization ðŸŒ\n",
            "2ï¸âƒ£ Transparency ðŸ“ˆ\n",
            "3ï¸âƒ£ Open-source codebase ðŸ“š\n",
            "4ï¸âƒ£ Community &amp; User Empowermentâœ… \n",
            "5ï¸âƒ£ Fair distribution ðŸ’°\n",
            "\n",
            "#XENcrypto #XENFT #XEN #ETH #NFT #Bitcoin #BNB #Crypto\n",
            "Cleaned :  solid first principle xen decentralization transparency opensource codebase community user empowerment fair distribution xencrypto xenft xen eth nft bitcoin bnb crypto\n",
            "==== Sample 1 ====\n",
            "Original:  $Bitcoin TO $100,000 SOONER THAN YOU THINKâ€¼ï¸ðŸ’¯ðŸ™\n",
            "\n",
            "#BitcoinÂ TO $100,000 WHETHER YOU BELIEVE OR NOTâ€¼ï¸ðŸ’¯ðŸ™\n",
            "\n",
            "$BTC #BitcoinÂ #BTCÂ Â Â \n",
            "\n",
            "#BitcoinÂ #BTCÂ #SHIB \n",
            "#HOGE #SAITAMA #BNBÂ Â Â #DOGE #ETH #BabyFloki #AltCoinSeason https://t.co/rtlFlKlVCv\n",
            "Cleaned :  bitcoin 100000 sooner think bitcoin 100000 whether believe btc bitcoin btc bitcoin btc shib hoge saitama bnb doge eth babyfloki altcoinseason\n",
            "==== Sample 2 ====\n",
            "Original:  Alright I have my rares. Who else is grabbing some of these @DogePunksBTC? Been in the discord a bit today and lovin the vibes. #Bitcoin #bitcoinordinals #crypto https://t.co/oaSNb6zOfK\n",
            "Cleaned :  alright rares else grabbing dogepunksbtc discord bit today lovin vibe bitcoin bitcoinordinals crypto\n",
            "==== Sample 3 ====\n",
            "Original:  Bitcoin (BTC) Targets Over $100,000 as This Important Pattern Reemerges, Analyst Says\n",
            "\n",
            "$Bitcoin TO $100,000 SOONER THAN YOU THINKâ€¼ï¸ðŸ’¯ðŸ™\n",
            "\n",
            "#BTC TO $100,000 WHETHER YOU BELIEVE OR NOTâ€¼ï¸ðŸ’¯ðŸ™\n",
            "\n",
            "#SHIB \n",
            "#HOGE #SAITAMA #BNBÂ Â Â #DOGE #ETH #BabyFloki #AltCoinSeason  https://t.co/gU71C732NS\n",
            "Cleaned :  bitcoin btc target 100000 important pattern reemerges analyst say bitcoin 100000 sooner think btc 100000 whether believe shib hoge saitama bnb doge eth babyfloki altcoinseason\n",
            "==== Sample 4 ====\n",
            "Original:  ðŸ“¢ Xverse Web-based pool is live:\n",
            "\n",
            "â€¢Update @xverseApp Chrome Browser Extension wallet to V0.6.2\n",
            "\n",
            "â€¢Stack @Stacks ðŸ‘‡\n",
            "\n",
            " https://t.co/s17rWwKnPD\n",
            "\n",
            "â€¢Earn #BTC Yield\n",
            "\n",
            "â€¢Continuous Stacking\n",
            "\n",
            "â€¢Stacking amount Top up at any moment of the cycle https://t.co/sBm89okDyb\n",
            "Cleaned :  xverse webbased pool live * update xverseapp chrome browser extension wallet v062 * stack stack * earn btc yield * continuous stacking * stacking amount top moment cycle\n",
            "==== Sample 5 ====\n",
            "Original:  Yesterday, a Bitcoin projection was displayed on the European Central Bank building in Frankfurt, Germany.\n",
            "ðŸ¥°ðŸ¥°\n",
            "#Bitcoin https://t.co/rUEdQxZmgU\n",
            "Cleaned :  yesterday bitcoin projection displayed european central bank building frankfurt germany bitcoin\n",
            "==== Sample 6 ====\n",
            "Original:  Unpopular opinion:\n",
            "\n",
            "This pump isnâ€™t going to stop and we are entering a full fledged bull run.\n",
            "\n",
            "HOWEVER, there will be a major sell off sometime between April and June next year.\n",
            "#Crypto #Bitcoin #Investing https://t.co/4fOMtYvY3c\n",
            "Cleaned :  unpopular opinion pump n't going stop entering full fledged bull run however major sell sometime april june next year crypto bitcoin investing\n",
            "==== Sample 7 ====\n",
            "Original:  #Bitcoin fixes this\n",
            "Cleaned :  bitcoin fix\n",
            "==== Sample 8 ====\n",
            "Original:  Solid bid in major ALT/BTC pairs today. \n",
            "\n",
            "If #Bitcoin continues to  consolidate up here, I'm expecting +15-20% across the wider altcoin market. https://t.co/YSiClL3Aza\n",
            "Cleaned :  solid bid major altbtc pair today bitcoin continues consolidate im expecting 1520 across wider altcoin market\n",
            "==== Sample 9 ====\n",
            "Original:  If you're filing 2022 taxes in the U.S., you should educate yourself on the tax implications of owning #bitcoin.\n",
            "\n",
            "@cdentbtc covers:\n",
            "- Bitcoin as property \n",
            "- Acquisitions vs. dispositions\n",
            "- Calculating capital gains\n",
            "- Bitcoin IRAs \n",
            "\n",
            "Watch the full video âž¡ï¸ https://t.co/3csLYodIgm https://t.co/i6IZHVuEpE\n",
            "Cleaned :  youre filing 2022 tax u educate tax implication owning bitcoin cdentbtc cover bitcoin property acquisition v disposition calculating capital gain bitcoin ira watch full video\n",
            "==== Sample 10 ====\n",
            "Original:  IBIS SACRED, new solana project. metaverse community\n",
            "LINK : https://t.co/LKBUnJkVIW\n",
            "\n",
            "Be part of Ibis Sacred Community \n",
            "\n",
            "@hey_wallet\n",
            "  send 13333 $FRONK to the first 500 retweets and follows \n",
            "\n",
            "#Solana #bitcoin #FRONK #BONK #CRYPTO #ETH #BSC #BNB #metaverse https://t.co/MOC2QOhfWU\n",
            "Cleaned :  ibis sacred new solana project metaverse community link part ibis sacred community heywallet send 13333 fronk first 500 retweets follows solana bitcoin fronk bonk crypto eth bsc bnb metaverse\n",
            "==== Sample 11 ====\n",
            "Original:  ðŸš¨ELIZABETH WARREN ANTI CRYPTO CAMPAIGN &amp; MICHAEL BURRY CAPITULATES TO BULLS\n",
            "\n",
            "WATCH â–¶ï¸ https://t.co/3LGvhfCEnX\n",
            "\n",
            "#ElizabethWarren #crypto #cryptocurrency #bitcoin #ethereum #xrp #ripple #hbar #hedera #nft #nfts #cryptonews #michaelburry https://t.co/JqzmSZN4AI\n",
            "Cleaned :  elizabeth warren anti crypto campaign michael burry capitulates bull watch elizabethwarren crypto cryptocurrency bitcoin ethereum xrp ripple hbar hedera nft nfts cryptonews michaelburry\n",
            "==== Sample 12 ====\n",
            "Original:  #Bitcoin Somethig different https://t.co/UIaFIRpNL5\n",
            "Cleaned :  bitcoin somethig different\n",
            "==== Sample 13 ====\n",
            "Original:  They want riots. Give them bank runs instead.\n",
            "\n",
            "#Silver #Gold #Bitcoin \n",
            "#IStandWithTrump https://t.co/wBQupqFyuj\n",
            "Cleaned :  want riot give bank run instead silver gold bitcoin istandwithtrump\n",
            "==== Sample 14 ====\n",
            "Original:  Free #NFT GiveawayðŸŽ\n",
            "\n",
            "ðŸš¨Drop your $ETH address wallet.\n",
            "\n",
            "ðŸš¨Follow me and Retweet.\n",
            "#nfts #nft #nftart #nftcommunity #nftcollector #nftartist #digitalart #crypto #cryptoart #art #ethereum #opensea #nftcollectors #blockchain #nftdrop #nftcollectibles #cryptocurrency #Bitcoin https://t.co/ZCFUoUQH1d\n",
            "Cleaned :  free nft giveaway drop eth address wallet follow retweet nfts nft nftart nftcommunity nftcollector nftartist digitalart crypto cryptoart art ethereum opensea nftcollectors blockchain nftdrop nftcollectibles cryptocurrency bitcoin\n",
            "==== Sample 15 ====\n",
            "Original:  Another day. Another orange pill. #Bitcoin https://t.co/dxVXnwVIF9\n",
            "Cleaned :  another day another orange pill bitcoin\n",
            "==== Sample 16 ====\n",
            "Original:  #Bitcoin I'm Satoshi âš¡ï¸â‚¿â‚¿â‚¿âš¡ï¸ https://t.co/7yL8JZ1D9U\n",
            "Cleaned :  bitcoin im satoshi BTCBTCBTC\n",
            "==== Sample 17 ====\n",
            "Original:  \"Meet quirky llama Prairie who loves making people laugh. A beauty treatment gone wrong left her with a blotchy pink face. Prairie turned it around, joked about her 'rosy cheeks' and became a beloved figure known for her humor &amp; iconic pink face.\" #btc #ordinal #122BitcoinLLaMaZ https://t.co/79WWb2qBWw\n",
            "Cleaned :  meet quirky llama prairie love making people laugh beauty treatment gone wrong left blotchy pink face prairie turned around joked rosy cheek became beloved figure known humor iconic pink face btc ordinal 122bitcoinllamaz\n",
            "==== Sample 18 ====\n",
            "Original:  ðŸ“ˆ #Bitcoin $BTC Number of Non-Zero Addresses just reached an ATH of 45,389,328\n",
            "\n",
            "Previous ATH of 45,388,865 was observed on 27 March 2023\n",
            "\n",
            "View metric:\n",
            "https://t.co/VtoChZbLsa https://t.co/35m9cdKa9T\n",
            "Cleaned :  bitcoin btc number nonzero address reached ath 45389328 previous ath 45388865 observed 27 march 2023 view metric\n",
            "==== Sample 19 ====\n",
            "Original:  ðŸ“ˆ #Bitcoin $BTC Percent Supply Last Active 3+ Years just reached an ATH of 39.733%\n",
            "\n",
            "View metric:\n",
            "https://t.co/ari6mxFMpi https://t.co/bQmNzrZPF8\n",
            "Cleaned :  bitcoin btc percent supply last active 3 year reached ath 39733 view metric\n",
            "==== Sample 20 ====\n",
            "Original:  Did you know that #litecoin is 2nd to #bitcoin and ahead of #ethereum on Newly Funded Addresses!? https://t.co/Ci0A2m23Xr\n",
            "Cleaned :  know litecoin 2nd bitcoin ahead ethereum newly funded address\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Show original vs cleaned for test set\n",
        "for i in range(20):\n",
        "    print(\"==== Sample\", i+1, \"====\")\n",
        "    print(\"Original: \", df_test['content'].iloc[i])\n",
        "    print(\"Cleaned : \", df_test['clean_content'].iloc[i])\n",
        "\n",
        "# training set\n",
        "for i in range(20):\n",
        "    print(\"==== Sample\", i+1, \"====\")\n",
        "    print(\"Original: \", df_train['content'].iloc[i])\n",
        "    print(\"Cleaned : \", df_train['clean_content'].iloc[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4e613e17",
      "metadata": {
        "id": "4e613e17"
      },
      "outputs": [],
      "source": [
        "# sentiment dictionary prediction\n",
        "\n",
        "df_train['sentiment_score'] = df_train['clean_content'].apply(get_sentiment_score)\n",
        "df_test['sentiment_score'] = df_test['clean_content'].apply(get_sentiment_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "09b17b7c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "09b17b7c",
        "outputId": "437ce27b-edc5-4ba7-c527-d0d4b7816eb5"
      },
      "outputs": [],
      "source": [
        "# Assess lexicon-based sentiment classifier\n",
        "ytest = df_train['sentiment']\n",
        "yhat = df_train['sentiment_score']\n",
        "auc_dict, acc_dict, cmat_dict, prec_dict, rec_dict, f1_dict = assess_binary_classifier(ytest=df_train['sentiment'], yhat=df_train['sentiment_score'],plot_roc=False ,cut_off=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15c2809c",
      "metadata": {
        "id": "15c2809c"
      },
      "source": [
        "From the confusion matrxix , we can easily get the following information\n",
        "the model predicted positive sentiment, but the actual sentiment was negative(type I error)\n",
        "True positive sentiment when the actual sentiment was also positive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6522e731",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6522e731",
        "outputId": "801dee6e-b34f-4d54-b2c9-e75253933155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           sentiment_dictionary\n",
            "Accuracy               0.554667\n",
            "Precision              0.878082\n",
            "Recall                 0.525410\n",
            "F1                     0.657436\n",
            "AUC                    0.661705\n"
          ]
        }
      ],
      "source": [
        "# assesment relusts for the sentiment analysis;\n",
        "df_scores['sentiment_dictionary'] = [acc_dict, prec_dict, rec_dict, f1_dict, auc_dict]\n",
        "print(df_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f64a99cb",
      "metadata": {
        "id": "f64a99cb"
      },
      "source": [
        "the second model: TFIDF_XGBoost\n",
        "frequency_based features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b1707928",
      "metadata": {
        "id": "b1707928"
      },
      "outputs": [],
      "source": [
        "# TFIDF_XGBoost\n",
        "tfidf_vectorizer = get_tfidf_vectorizer()\n",
        "#TFIDF Feature Extraction:\n",
        "reviews_clean_tfidf_tr = tfidf_vectorizer.fit_transform(df_train['clean_content']) #\n",
        "reviews_clean_tfidf_ts = tfidf_vectorizer.transform(df_test['clean_content'])\n",
        "\n",
        "## training the XGBoost\n",
        "y_train = df_train['sentiment']\n",
        "scale_pos_weight = len(y_train[y_train == False]) / len(y_train[y_train == True])\n",
        "xgbc = XGBClassifier(n_estimators=200, random_state=42,scale_pos_weight =scale_pos_weight)\n",
        "\n",
        "xgbc.fit(reviews_clean_tfidf_tr,y_train)\n",
        "xgbc_tfidf_discrete_pr=xgbc.predict(reviews_clean_tfidf_ts)\n",
        "xgbc_tfidf_proba_pr=xgbc.predict_proba(reviews_clean_tfidf_ts)[:,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "33M1apXOoqpr",
      "metadata": {
        "id": "33M1apXOoqpr"
      },
      "outputs": [],
      "source": [
        "y_train = df_train['sentiment'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8377f97c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8377f97c",
        "outputId": "ac419551-6cfe-46d8-d857-af83d1dc5681"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           sentiment_dictionary  TF-IDF_XGBoost\n",
            "Accuracy               0.554667        0.746000\n",
            "Precision              0.878082        0.821346\n",
            "Recall                 0.525410        0.876238\n",
            "F1                     0.657436        0.847904\n",
            "AUC                    0.661705        0.614222\n"
          ]
        }
      ],
      "source": [
        "#Assessment of TF_IDF_XGBoost\n",
        "\n",
        "y_test = df_test['sentiment']\n",
        "yhat_proba_xgb = xgbc_tfidf_proba_pr\n",
        "\n",
        "\n",
        "auc_xgb, acc_xgb, cmat_xgb, prec_xgb, rec_xgb, f1_xgb = assess_binary_classifier(\n",
        "    ytest=y_test,\n",
        "    yhat=yhat_proba_xgb,\n",
        "    cut_off=0.5,\n",
        "    plot_roc=False\n",
        ")\n",
        "\n",
        "df_scores['TF-IDF_XGBoost'] = [acc_xgb, prec_xgb, rec_xgb, f1_xgb, auc_xgb]\n",
        "print(df_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "657d8301",
      "metadata": {
        "id": "657d8301"
      },
      "source": [
        "Model 3: RNN classifer with LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8UEVUrLJNr5j",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UEVUrLJNr5j",
        "outputId": "aa709e91-47f7-4108-9d2c-0fb3be9103e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The longest review has 44 words.\n"
          ]
        }
      ],
      "source": [
        "# Tokenizer setup\n",
        "NUM_WORDS = 4000\n",
        "tok = Tokenizer(num_words=NUM_WORDS, oov_token=1)\n",
        "tok.fit_on_texts(df_train['clean_content'])  # åªå¯¹è®­ç»ƒé›†æ‹Ÿåˆ\n",
        "\n",
        "# Convert texts to integer sequences\n",
        "X_tr_int = tok.texts_to_sequences(df_train['clean_content'])\n",
        "X_ts_int = tok.texts_to_sequences(df_test['clean_content'])\n",
        "\n",
        "#  Set max review length\n",
        "MAX_REVIEW_LENGTH = max(len(seq) for seq in X_tr_int)  # æˆ–æ‰‹åŠ¨è®¾ä¸º 44\n",
        "print(f\"The longest review has {MAX_REVIEW_LENGTH} words.\")\n",
        "\n",
        "# Pad sequences to the same length\n",
        "X_tr_int_pad = pad_sequences(X_tr_int, maxlen=MAX_REVIEW_LENGTH)\n",
        "X_ts_int_pad = pad_sequences(X_ts_int, maxlen=MAX_REVIEW_LENGTH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "bJPIt0AFt0Oh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJPIt0AFt0Oh",
        "outputId": "b867f804-7e97-4d6b-c0a9-8fbc10b76639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.8394 - loss: 0.6534 - val_accuracy: 0.7520 - val_loss: 0.6659\n",
            "Epoch 2/5\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8335 - loss: 0.6546 - val_accuracy: 0.7520 - val_loss: 0.6545\n",
            "Epoch 3/5\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8345 - loss: 0.6402 - val_accuracy: 0.7520 - val_loss: 0.6347\n",
            "Epoch 4/5\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8373 - loss: 0.6231 - val_accuracy: 0.7493 - val_loss: 0.5926\n",
            "Epoch 5/5\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9220 - loss: 0.5436 - val_accuracy: 0.7627 - val_loss: 0.5132\n",
            "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "NN test set performance:\tAUC=0.7185\tAccuracy=0.8040\n",
            "Confusion matrix:\n",
            "[[ 14  82]\n",
            " [ 16 388]]\n"
          ]
        }
      ],
      "source": [
        "# Define hyperparameters\n",
        "NB_HIDDEN = 16\n",
        "EPOCH = 5\n",
        "BATCH_SIZE = 64\n",
        "EMBEDDING_DIM = 50\n",
        "VAL_SPLIT = 0.25\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Build the GRU model using the modular wrapper\n",
        "bi_gru = build_bidirectional_rnn_classifier(\n",
        "    vocab_size=NUM_WORDS,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    input_length=MAX_REVIEW_LENGTH,\n",
        "    rnn_type='gru',\n",
        "    hidden_units=NB_HIDDEN,\n",
        "    learning_rate=LEARNING_RATE\n",
        ")\n",
        "\n",
        "# fitting the model\n",
        "story = bi_gru.fit(X_tr_int_pad, y_train, batch_size=BATCH_SIZE, epochs=EPOCH, validation_split=VAL_SPLIT,verbose=1,\n",
        "                   class_weight=class_weights)\n",
        "\n",
        "\n",
        "# Assess the performance of the FFN\n",
        "acc_bigru, prec_bigru,camt_bigru, rec_bigru, f1_bigru, auc_bigru  = diag_nn(bi_gru, story, X_ts_int_pad, y_test, plot_roc=False, plot_loss=False)\n",
        "\n",
        "# Add results to our data frame to keep track of results\n",
        "df_scores['BiRNN_GRU'] = [acc_bigru, prec_bigru, rec_bigru, f1_bigru, auc_bigru]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "695fe7a8",
      "metadata": {
        "id": "695fe7a8"
      },
      "source": [
        "Model 4: RNN classifer with Bidirectional LSTM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "EdwvQ3P8vCej",
      "metadata": {
        "id": "EdwvQ3P8vCej"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start embedding process for 6555 words.\n",
            "Created embedding matrix of shape (4000, 100) in 0.00017738739649454752 min \n",
            "Encountered 4483 out-of-vocabulary words.\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.5368 - loss: 0.7696 - val_accuracy: 0.5173 - val_loss: 0.7371\n",
            "Epoch 2/5\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5242 - loss: 0.6641 - val_accuracy: 0.5547 - val_loss: 0.7127\n",
            "Epoch 3/5\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6710 - loss: 0.6088 - val_accuracy: 0.6133 - val_loss: 0.6841\n",
            "Epoch 4/5\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7336 - loss: 0.5841 - val_accuracy: 0.6427 - val_loss: 0.6829\n",
            "Epoch 5/5\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8027 - loss: 0.5109 - val_accuracy: 0.6613 - val_loss: 0.6393\n",
            "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "NN test set performance:\tAUC=0.5450\tAccuracy=0.6500\n",
            "Confusion matrix:\n",
            "[[ 33  63]\n",
            " [112 292]]\n"
          ]
        }
      ],
      "source": [
        "# Create embedding weight matrix\n",
        "imdb_index = w2v_kv\n",
        "imdb_embeddings, _ = get_embedding_matrix(tok, imdb_index, NUM_WORDS)\n",
        "\n",
        "\n",
        "# hyperparameter setting\n",
        "NB_HIDDEN = 16\n",
        "EPOCH = 5\n",
        "BATCH_SIZE = 64\n",
        "EMBEDDING_DIM = 100  # Changed to 100 to match pre-trained embedding dimension\n",
        "VAL_SPLIT = 0.25\n",
        "\n",
        "# Model \n",
        "model_lstm = build_bidirectional_rnn_classifier(\n",
        "    vocab_size=NUM_WORDS,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    input_length=MAX_REVIEW_LENGTH,\n",
        "    rnn_type='lstm',\n",
        "    hidden_units=NB_HIDDEN,\n",
        "    learning_rate=0.001,\n",
        "    embedding_weights=imdb_embeddings,  # Pass the pre-trained embeddings\n",
        "    trainable_embedding=True\n",
        "\n",
        ")\n",
        "\n",
        "# Training\n",
        "RNN_lstm = model_lstm.fit(\n",
        "    X_tr_int_pad,          # padded train sequences\n",
        "    y_train,           # labels\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCH,\n",
        "    validation_split=VAL_SPLIT,\n",
        "    verbose=1,\n",
        "    class_weight=class_weights\n",
        ")\n",
        "\n",
        "\n",
        "# Assess the performance of the FFN\n",
        "acc_bilstm, prec_bilstm,camt_bilstm, rec_bilstm, f1_bilstm, auc_bilstm  = diag_nn(model_lstm, RNN_lstm, X_ts_int_pad, y_test, plot_roc=False, plot_loss=False)\n",
        "\n",
        "# Add results to our data frame to keep track of results\n",
        "df_scores['BiRNN_LSTM'] = [acc_bilstm, prec_bilstm, rec_bilstm, f1_bilstm, auc_bilstm]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f80d041",
      "metadata": {
        "id": "1f80d041"
      },
      "source": [
        "Model 5. Pre-trained transformer model(huggingface pipelines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "52ec7e19",
      "metadata": {
        "id": "52ec7e19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        }
      ],
      "source": [
        "df_train['sentiment'] = df_train['sentiment'].astype(int)\n",
        "df_test['sentiment'] = df_test['sentiment'].astype(int)\n",
        "\n",
        "classifier = pipeline('sentiment-analysis', model='distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n",
        "                      torch_dtype=None) # Removed device=-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f8a9601",
      "metadata": {
        "id": "4f8a9601"
      },
      "outputs": [],
      "source": [
        "#  Huggingface pipeline prediction\n",
        "labels_test, probs_test = pipeline_classify(df_test['clean_content'].tolist())\n",
        "\n",
        "# store df_test\n",
        "df_test['pipeline_label'] = labels_test\n",
        "df_test['pipeline_prob'] = probs_test\n",
        "\n",
        "\n",
        "y_true = df_test['sentiment']\n",
        "y_score = df_test['pipeline_prob']\n",
        "\n",
        "\n",
        "auc_hugging, acc_hugging, cmat_hugging, prec_hugging, rec_hugging, f1_hugging = assess_binary_classifier(\n",
        "    ytest = y_true,\n",
        "    yhat = y_score,\n",
        "    cut_off=0.5,\n",
        "    plot_roc=False\n",
        ")\n",
        "#  df_scores\n",
        "df_scores['Huggingface_pipeline'] = [acc_hugging, prec_hugging, rec_hugging, f1_hugging, auc_hugging]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44d77a56",
      "metadata": {
        "id": "44d77a56"
      },
      "source": [
        "Model 6. Fine-tune a pre-trained transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "n1wcJFn8x-Tj",
      "metadata": {
        "id": "n1wcJFn8x-Tj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2449881018ea478f82fe1c82507edd56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/560 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e24000321c3b493292975a9139afe828",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "828124416f5d423cace69fad87342300",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Casting the dataset:   0%|          | 0/560 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1f73e161938427184bf4b18f19521e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Casting the dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# \n",
        "df_pos = df_train[df_train['sentiment'] == 1]\n",
        "df_neg = df_train[df_train['sentiment'] == 0]\n",
        "\n",
        "df_pos_sampled = df_pos.sample(n=len(df_neg), random_state=42)\n",
        "df_balanced = pd.concat([df_neg, df_pos_sampled]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# convert to Huggingface Dataset format\n",
        "train_data = df_balanced[['clean_content', 'sentiment']].rename(columns={'clean_content': 'text', 'sentiment': 'label'})\n",
        "test_data = df_test[['clean_content', 'sentiment']].rename(columns={'clean_content': 'text', 'sentiment': 'label'})\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_data)\n",
        "test_dataset = Dataset.from_pandas(test_data)\n",
        "\n",
        "\n",
        "#tokenizer + map\n",
        "\n",
        "from transformers import DistilBertTokenizerFast\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize(batch):\n",
        "    tokens = tokenizer(batch['text'], truncation=True, padding=\"max_length\", max_length=44)\n",
        "    tokens[\"label\"] = batch[\"label\"]  # æ˜¾å¼ä¿ç•™æ ‡ç­¾\n",
        "    return tokens\n",
        "\n",
        "train_tokenized = train_dataset.map(tokenize, batched=True)\n",
        "test_tokenized = test_dataset.map(tokenize, batched=True)\n",
        "\n",
        "\n",
        "class_label = ClassLabel(num_classes=2, names=[\"negative\", \"positive\"])\n",
        "\n",
        "train_tokenized = train_tokenized.cast_column(\"label\", class_label)\n",
        "test_tokenized = test_tokenized.cast_column(\"label\", class_label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ntrn8cvy3H-U",
      "metadata": {
        "id": "ntrn8cvy3H-U"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [105/105 01:16, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.686100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.667800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.598700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.520400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.424000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.344700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.408000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.258200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.238900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.231300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Call the fine-tuning function\n",
        "trainer, model = fine_tune_distilbert(\n",
        "    train_dataset=train_tokenized,\n",
        "    test_dataset=test_tokenized,\n",
        "    num_train_epochs=2\n",
        ")\n",
        "#prediction on test set\n",
        "predictions = trainer.predict(test_tokenized)\n",
        "probs = predictions.predictions\n",
        "y_pred_prob = probs[:, 1] \n",
        " \n",
        "#Evaluate using custom evaluation function\n",
        "auc_bert, acc_bert, cmat_bert, prec_bert, rec_bert, f1_bert = assess_binary_classifier(\n",
        "    ytest = df_test['sentiment'], \n",
        "    yhat = y_pred_prob,\n",
        "    cut_off = 0.5,\n",
        "    plot_roc=False\n",
        ")\n",
        "\n",
        "# Store results into the summary table\n",
        "df_scores['DistilBERT_finetuned'] = [acc_bert, prec_bert, rec_bert, f1_bert, auc_bert]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c11b61",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           sentiment_dictionary  TF-IDF_XGBoost  BiRNN_GRU  BiRNN_LSTM  \\\n",
            "Accuracy               0.554667        0.746000   0.715011    0.610174   \n",
            "Precision              0.878082        0.821346   0.794000    0.680000   \n",
            "Recall                 0.525410        0.876238   0.838202    0.838889   \n",
            "F1                     0.657436        0.847904   0.923267    0.747525   \n",
            "AUC                    0.661705        0.614222   0.878681    0.790576   \n",
            "\n",
            "           Huggingface_pipeline  DistilBERT_finetuned  \n",
            "Accuracy               0.398000              0.722000  \n",
            "Precision              0.981308              0.985348  \n",
            "Recall                 0.259901              0.665842  \n",
            "F1                     0.410959              0.794682  \n",
            "AUC                    0.762144              0.913676  \n"
          ]
        }
      ],
      "source": [
        "print(df_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "872a40de",
      "metadata": {},
      "outputs": [],
      "source": [
        "#           sentiment_dictionary  TF-IDF_XGBoost  BiRNN_GRU  BiRNN_LSTM  \\\n",
        "#Accuracy               0.554667        0.746000   0.715011    0.610174   \n",
        "#Precision              0.878082        0.821346   0.794000    0.680000   \n",
        "#Recall                 0.525410        0.876238   0.838202    0.838889   \n",
        "#F1                     0.657436        0.847904   0.923267    0.747525   \n",
        "#AUC                    0.661705        0.614222   0.878681    0.790576   \n",
        "\n",
        "#           Huggingface_pipeline  DistilBERT_finetuned  \n",
        "#Accuracy               0.398000              0.722000  \n",
        "#Precision              0.981308              0.985348  \n",
        "#Recall                 0.259901              0.665842  \n",
        "#F1                     0.410959              0.794682  \n",
        "#AUC                    0.762144              0.913676"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e25acb2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip freeze > requirements.txt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
